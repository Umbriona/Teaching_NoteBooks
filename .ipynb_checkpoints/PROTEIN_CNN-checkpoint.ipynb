{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enzyme optimal temperature prediction using CNN \n",
    "\n",
    "In this note book we will construct a model that predicts the optimal temperature for enzymes. The data set we will be using consists of protein enzymes anotated with experimentaly measured optimal temperatures. We will need some simple preprocessing to make the data interpretable by the model. The model will consist of an embedding layer, convolutional layers and at the end a prediction layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from Bio import SeqIO\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from utils import Preprocessing  as pre\n",
    "from utils import test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "The data is in the form of fasta files. The sequences is coded as a strings with each amino acid encoded as a letter in the alphabet <span style=\"color:red\"> \"ACDEFGHIKLMNPQRSTVWY\"</span>. For the model to be able to read in the data we need to encode the residues not in the alphabet but as integers. The different sequences is of different lengths which also needs to be dealth with. The easiest way to do this is to padd the sequence to a specified max length at the end with an integer other then those used to code for the amino acids. In the function to_int in the Preprocessing module all amino acids are coded with integers ranging from 0-19, the padding and any unrecogniced amino acid are coded as integer 20.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">K9L4P7 37.0\r\n",
      "MKSTIITSILFSVATVQAYSPAEQIDVQSHLLSDPTKVEGKTYDYVIAGGGLTGLTVASK\r\n",
      "LSENPKIKVLVIEKGFYESNDGPIIEDPNAYGEIFGTSVDQNYLTVPLINNRTGEIKSGL\r\n",
      "GLGGSTLINGDSWTRPDKVQIDSWEKVFGMEGWNWDNVFQYMQKAERSRPPTAAQIEAGH\r\n",
      "FYDPACHGTDGTVHAGPRDNGKPWSPLMRALMNTVSAFGVPVQKDFHCGHPRGVSMIPNN\r\n",
      "LHENQIRADAAREWLLPNYQRDNLQILTGQKVGKVLFNQTASGPKAVGVNFGTNKAVNFN\r\n",
      "VYAKQEVLLAAGSAISPLILEYSGIGIKSVLDKAGVKQLLELPVGLNMQDQTTTTVRSRA\r\n",
      "NNAPGQGQAAYFANFTEVLGDHAAQGINLLDTKLDQWAEETVARGGFHNVTALKIQYENY\r\n",
      "RNWLLDEDVAFAELFFDTEGKINFDIWNLIPFTRGSVHILSSDPYLWQYANDPKFFMNEL\r\n",
      "DLLGQAAATKLGRELSSAGEMKKYYAGETIPGDNLPQDATVEQWEDYVMMNFRPNWHAVS\r\n",
      "TCSMMSRELGGVVDATAKVYGTQGLRVIDGSIPPTQVSSHVMTVFYGMALRIAESVLEDY\r\n",
      "AKSA\r\n",
      ">Q9L7P2 42.0\r\n",
      "MQTPKLIRPTLLSMAILSSMAWATGASAALVPPKGYDAPIEKMKTGDHNFSCEAIPKPYT\r\n",
      "DKLVFRSKYEGSDKARATLNAVSEEAFRDATKDITTLERGVSKVVMQYMRDGRPEQLDCA\r\n",
      "LNMMTTWAKADALESREFNHTGKSMRKWALGSMSSAYLRLKFSESHPLANRQQDAKIIET\r\n",
      "WFSKLADQVVSDWSNLPLEKINNHSYWAAWSVMATAVATNRQDLFDWAVKEYKVAANQVD\r\n",
      "KDGFLPNEMKRRQRALSYHNYALPPLAMIASFAQANGVDLRPENNGALKRLGDRVLAGVK\r\n",
      "DPSIFAEHNGEKQDMTDLKKDPKFAWLEPYCSLYTCSPDVLEEKHEKQPFKTFRLGGDLT\r\n",
      "KVYDPTHEKGDKGDNDGS\r\n"
     ]
    }
   ],
   "source": [
    "# First look at raw data \n",
    "! head --lines=20 data/topt.fasta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import and preprocess data with module to_int in Preprocessing module\n",
    "\n",
    "MAX_SEQ_LEN = 512\n",
    "\n",
    "data = {'id': [], 'seq': [], 'seq_int': [], 'prop': []}\n",
    "for i, rec in enumerate(SeqIO.parse('data/topt.fasta','fasta')):\n",
    "    if len(rec.seq) > MAX_SEQ_LEN:  # Filter out sequences longer then specified by MAX_SEQ_LEN\n",
    "        continue\n",
    "    data['id'].append(rec.id)\n",
    "    data['seq'].append(rec.seq)\n",
    "    data['seq_int'].append(pre.to_int(rec.seq, MAX_SEQ_LEN))\n",
    "    data['prop'].append(np.array(rec.description.split()[-1], dtype=np.float32)) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1\n",
    "Instead of padding with a separate integer it is also common to repeat shorter sequences to the specified max length. This can be helpfull when using BatchNormalization in your model as the different length padding can distort the statistics of the sequences. This functionality is not present in the module Preprocessing, instead you will implement this function below.\n",
    "\n",
    "Specificaly: create a function that repeats sequences that is shorter then the max length specified. When not a whole sequnce can be repeated within the max length choos only the first part of the sequence that fitts te missing part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeat(seq, max_length):\n",
    "    return pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing repeat module. Too Do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id:  Q9L7P2\n",
      "seq:  MQTPKLIRPTLLSMAILSSMAWATGASAALVPPKGYDAPIEKMKTGDHNFSCEAIPKPYTDKLVFRSKYEGSDKARATLNAVSEEAFRDATKDITTLERGVSKVVMQYMRDGRPEQLDCALNMMTTWAKADALESREFNHTGKSMRKWALGSMSSAYLRLKFSESHPLANRQQDAKIIETWFSKLADQVVSDWSNLPLEKINNHSYWAAWSVMATAVATNRQDLFDWAVKEYKVAANQVDKDGFLPNEMKRRQRALSYHNYALPPLAMIASFAQANGVDLRPENNGALKRLGDRVLAGVKDPSIFAEHNGEKQDMTDLKKDPKFAWLEPYCSLYTCSPDVLEEKHEKQPFKTFRLGGDLTKVYDPTHEKGDKGDNDGS\n",
      "seq int: [10. 13. 16. 12.  8.  9.  7. 14. 12. 16.  9.  9. 15. 10.  0.  7.  9. 15.\n",
      " 15. 10.  0. 18.  0. 16.  5.  0. 15.  0.  0.  9. 17. 12. 12.  8.  5. 19.\n",
      "  2.  0. 12.  7.  3.  8. 10.  8. 16.  5.  2.  6. 11.  4. 15.  1.  3.  0.\n",
      "  7. 12.  8. 12. 19. 16.  2.  8.  9. 17.  4. 14. 15.  8. 19.  3.  5. 15.\n",
      "  2.  8.  0. 14.  0. 16.  9. 11.  0. 17. 15.  3.  3.  0.  4. 14.  2.  0.\n",
      " 16.  8.  2.  7. 16. 16.  9.  3. 14.  5. 17. 15.  8. 17. 17. 10. 13. 19.\n",
      " 10. 14.  2.  5. 14. 12.  3. 13.  9.  2.  1.  0.  9. 11. 10. 10. 16. 16.\n",
      " 18.  0.  8.  0.  2.  0.  9.  3. 15. 14.  3.  4. 11.  6. 16.  5.  8. 15.\n",
      " 10. 14.  8. 18.  0.  9.  5. 15. 10. 15. 15.  0. 19.  9. 14.  9.  8.  4.\n",
      " 15.  3. 15.  6. 12.  9.  0. 11. 14. 13. 13.  2.  0.  8.  7.  7.  3. 16.\n",
      " 18.  4. 15.  8.  9.  0.  2. 13. 17. 17. 15.  2. 18. 15. 11.  9. 12.  9.\n",
      "  3.  8.  7. 11. 11.  6. 15. 19. 18.  0.  0. 18. 15. 17. 10.  0. 16.  0.\n",
      " 17.  0. 16. 11. 14. 13.  2.  9.  4.  2. 18.  0. 17.  8.  3. 19.  8. 17.\n",
      "  0.  0. 11. 13. 17.  2.  8.  2.  5.  4.  9. 12. 11.  3. 10.  8. 14. 14.\n",
      " 13. 14.  0.  9. 15. 19.  6. 11. 19.  0.  9. 12. 12.  9.  0. 10.  7.  0.\n",
      " 15.  4.  0. 13.  0. 11.  5. 17.  2.  9. 14. 12.  3. 11. 11.  5.  0.  9.\n",
      "  8. 14.  9.  5.  2. 14. 17.  9.  0.  5. 17.  8.  2. 12. 15.  7.  4.  0.\n",
      "  3.  6. 11.  5.  3.  8. 13.  2. 10. 16.  2.  9.  8.  8.  2. 12.  8.  4.\n",
      "  0. 18.  9.  3. 12. 19.  1. 15.  9. 19. 16.  1. 15. 12.  2. 17.  9.  3.\n",
      "  3.  8.  6.  3.  8. 13. 12.  4.  8. 16.  4. 14.  9.  5.  5.  2.  9. 16.\n",
      "  8. 17. 19.  2. 12. 16.  6.  3.  8.  5.  2.  8.  5.  2. 11.  2.  5. 15.\n",
      " 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20.\n",
      " 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20.\n",
      " 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20.\n",
      " 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20.\n",
      " 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20.\n",
      " 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20.\n",
      " 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20.\n",
      " 20. 20. 20. 20. 20. 20. 20. 20.]\n",
      "Shape of data: (512,)\n",
      "Temp:  42.0\n",
      "id:  Q5JDG9\n",
      "seq:  MKLDVIGIGNLNYDIIFTLERFPEFHEKINARGAHFGLGGAAANTISWLAHFGLKTGYIGAVGNDDVGEMHIKYFQGIGVDTGGIDVVEEPSGVAVAMVAGDDKRIVKYPGANLRRRFKPEYASRAKFLHLSSNPPELIEEAVNFASQRGIKVSLDIGEAPLPRELESKVDYLMMNEDEYRRKYGSLDPSLCRAKNLVVTLNGGGALVREGDNVFEVRGLSAKVVDSTGAGDSFDAGVIYGVLNGWSLLDSAKLGMLLAYLTVQKVGARSAIVPLEEVKRIAREVGLDLPFNRT\n",
      "seq int: [10.  8.  9.  2. 17.  7.  5.  7.  5. 11.  9. 11. 19.  2.  7.  7.  4. 16.\n",
      "  9.  3. 14.  4. 12.  3.  4.  6.  3.  8.  7. 11.  0. 14.  5.  0.  6.  4.\n",
      "  5.  9.  5.  5.  0.  0.  0. 11. 16.  7. 15. 18.  9.  0.  6.  4.  5.  9.\n",
      "  8. 16.  5. 19.  7.  5.  0. 17.  5. 11.  2.  2. 17.  5.  3. 10.  6.  7.\n",
      "  8. 19.  4. 13.  5.  7.  5. 17.  2. 16.  5.  5.  7.  2. 17. 17.  3.  3.\n",
      " 12. 15.  5. 17.  0. 17.  0. 10. 17.  0.  5.  2.  2.  8. 14.  7. 17.  8.\n",
      " 19. 12.  5.  0. 11.  9. 14. 14. 14.  4.  8. 12.  3. 19.  0. 15. 14.  0.\n",
      "  8.  4.  9.  6.  9. 15. 15. 11. 12. 12.  3.  9.  7.  3.  3.  0. 17. 11.\n",
      "  4.  0. 15. 13. 14.  5.  7.  8. 17. 15.  9.  2.  7.  5.  3.  0. 12.  9.\n",
      " 12. 14.  3.  9.  3. 15.  8. 17.  2. 19.  9. 10. 10. 11.  3.  2.  3. 19.\n",
      " 14. 14.  8. 19.  5. 15.  9.  2. 12. 15.  9.  1. 14.  0.  8. 11.  9. 17.\n",
      " 17. 16.  9. 11.  5.  5.  5.  0.  9. 17. 14.  3.  5.  2. 11. 17.  4.  3.\n",
      " 17. 14.  5.  9. 15.  0.  8. 17. 17.  2. 15. 16.  5.  0.  5.  2. 15.  4.\n",
      "  2.  0.  5. 17.  7. 19.  5. 17.  9. 11.  5. 18. 15.  9.  9.  2. 15.  0.\n",
      "  8.  9.  5. 10.  9.  9.  0. 19.  9. 16. 17. 13.  8. 17.  5.  0. 14. 15.\n",
      "  0.  7. 17. 12.  9.  3.  3. 17.  8. 14.  7.  0. 14.  3. 17.  5.  9.  2.\n",
      "  9. 12.  4. 11. 14. 16. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20.\n",
      " 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20.\n",
      " 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20.\n",
      " 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20.\n",
      " 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20.\n",
      " 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20.\n",
      " 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20.\n",
      " 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20.\n",
      " 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20.\n",
      " 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20.\n",
      " 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20.\n",
      " 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20.\n",
      " 20. 20. 20. 20. 20. 20. 20. 20.]\n",
      "Shape of data: (512,)\n",
      "Temp:  85.0\n"
     ]
    }
   ],
   "source": [
    "#Printing data 0:2\n",
    "for i in range(2):\n",
    "    print(\"id: \",data['id'][i])\n",
    "    print(\"seq: \", data['seq'][i])\n",
    "    print(\"seq int:\", data['seq_int'][i])\n",
    "    print(\"Shape of data:\", data['seq_int'][i].shape)\n",
    "    print(\"Temp: \", data['prop'][i])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spliting data \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split( np.array(data['seq_int']), np.array(data['prop']), test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of examples in the test set         1048\n",
      "Number of examples in the validation set    262\n",
      "Number of examples in the test set         1048\n",
      "Number of examples in the validation set    262\n"
     ]
    }
   ],
   "source": [
    "# Printing the size of the data\n",
    "\n",
    "print('Number of examples in the test set %12s' % len(X_train))\n",
    "print('Number of examples in the validation set %6s' % len(X_test))\n",
    "\n",
    "print('Number of examples in the test set %12s' % len(y_train))\n",
    "print('Number of examples in the validation set %6s' % len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Developement \n",
    "We will investigate the diferent components (Layers) separatly then we will string them together in to a model that predicts the optimal temperature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding layer\n",
    "Embedding is just a way to reprecent our integer residues as vectors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 512)\n",
      "(1, 512, 10)\n",
      "[[[ 0.00023736 -0.03861759  0.04725392 ...  0.00440492  0.03163436\n",
      "   -0.00851383]\n",
      "  [-0.03545252  0.02851243  0.04858854 ...  0.01048476  0.01747711\n",
      "    0.03319747]\n",
      "  [-0.03196963  0.04438924  0.01361874 ... -0.01556272 -0.02589515\n",
      "    0.04253567]\n",
      "  ...\n",
      "  [-0.03324833 -0.0196014   0.02365692 ...  0.02031409 -0.00173908\n",
      "    0.03389896]\n",
      "  [-0.03324833 -0.0196014   0.02365692 ...  0.02031409 -0.00173908\n",
      "    0.03389896]\n",
      "  [-0.03324833 -0.0196014   0.02365692 ...  0.02031409 -0.00173908\n",
      "    0.03389896]]]\n"
     ]
    }
   ],
   "source": [
    "# Embedding layers\n",
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "VOCABELARY_SIZE = 21\n",
    "EMBEDDING_SIZE = 10\n",
    "\n",
    "embedding_layer = Embedding(VOCABELARY_SIZE, EMBEDDING_SIZE)\n",
    "emb_X1 = embedding_layer(X_train[0].reshape([-1,MAX_SEQ_LEN,]))\n",
    "print(X_train[0].reshape([1,MAX_SEQ_LEN,]).shape)\n",
    "print(emb_X1.shape)\n",
    "print(emb_X1.numpy()[:10,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 503, 32)\n",
      "(1, 512, 32)\n",
      "(1, 256, 32)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Conv1D\n",
    "\n",
    "KERNEL_SIZE = 10\n",
    "FILTERS = 32\n",
    "\n",
    "# Convolutions with no padding stepsize 1\n",
    "conv_no_padding = Conv1D(FILTERS, KERNEL_SIZE, strides = 1, padding = 'valid')\n",
    "# Convolutions with same padding stepsize 1\n",
    "conv_same_padding = Conv1D(FILTERS, KERNEL_SIZE, strides = 1, padding = 'same')\n",
    "# Convolutions with same padding stepsize 2\n",
    "conv_step_2 = Conv1D(FILTERS, KERNEL_SIZE, strides = 2, padding = 'same')\n",
    "\n",
    "conv_X1_s1 = conv_no_padding(emb_X1)\n",
    "conv_X1_same = conv_same_padding(emb_X1)\n",
    "conv_X1_s2 = conv_step_2(emb_X1)\n",
    "\n",
    "\n",
    "print(conv_no_padding(emb_X1).numpy().shape)\n",
    "print(conv_same_padding(emb_X1).numpy().shape)\n",
    "print(conv_step_2(emb_X1).numpy().shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Normalisation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 1.  2.  3.]\n",
      "  [ 4.  5.  6.]\n",
      "  [ 7.  8.  9.]]\n",
      "\n",
      " [[ 2.  3.  4.]\n",
      "  [ 5.  6.  7.]\n",
      "  [ 8.  9. 10.]]]\n",
      "[[[-1.3998879  -1.399888   -1.399888  ]\n",
      "  [-0.19998395 -0.19998407 -0.19998407]\n",
      "  [ 0.9999201   0.9999199   0.9999199 ]]\n",
      "\n",
      " [[-0.99991995 -0.9999201  -0.99992   ]\n",
      "  [ 0.19998407  0.19998384  0.19998407]\n",
      "  [ 1.399888    1.3998878   1.399888  ]]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import BatchNormalization\n",
    "\n",
    "bn = BatchNormalization()\n",
    "\n",
    "const1 = tf.constant([[[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]],\n",
    "                     [[2.0, 3.0, 4.0], [5.0, 6.0, 7.0], [8.0, 9.0, 10.0]]], dtype=tf.float32)\n",
    "const1 = tf.reshape(const1, shape=(-1,3,3))\n",
    "\n",
    "\n",
    "const_normalized = bn(const1, training=True)\n",
    "\n",
    "print(const1.numpy())\n",
    "print(const_normalized.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 1.  2.  3.]\n",
      "  [ 4.  5.  6.]\n",
      "  [ 7.  8.  9.]]\n",
      "\n",
      " [[ 2.  3.  4.]\n",
      "  [ 5.  6.  7.]\n",
      "  [ 8.  9. 10.]]]\n",
      "tf.Tensor(\n",
      "[[[-1.2238274  0.         1.2238274]\n",
      "  [-1.2238274  0.         1.2238274]\n",
      "  [-1.2238274  0.         1.2238274]]\n",
      "\n",
      " [[-1.2238274  0.         1.2238274]\n",
      "  [-1.2238274  0.         1.2238274]\n",
      "  [-1.2238274  0.         1.2238274]]], shape=(2, 3, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import LayerNormalization\n",
    "\n",
    "ln = LayerNormalization()\n",
    "const2 = tf.constant([[[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]],\n",
    "                     [[2.0, 3.0, 4.0], [5.0, 6.0, 7.0], [8.0, 9.0, 10.0]]], dtype=tf.float32)\n",
    "const2_normalized = ln(const2)\n",
    "\n",
    "print(const2.numpy())\n",
    "print(const2_normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0.  0.  6.]\n",
      "  [ 8. 10. 12.]\n",
      "  [14.  0. 18.]]\n",
      "\n",
      " [[ 0.  0.  0.]\n",
      "  [ 0.  0. 14.]\n",
      "  [ 0. 18. 20.]]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "dp = Dropout(0.5)\n",
    "const2 = tf.constant([[[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]],\n",
    "                     [[2.0, 3.0, 4.0], [5.0, 6.0, 7.0], [8.0, 9.0, 10.0]]], dtype=tf.float32)\n",
    "out_dp = dp(const2, training=True)\n",
    "\n",
    "print(out_dp.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flatten "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 8192)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Flatten\n",
    "\n",
    "flat = Flatten()\n",
    "\n",
    "flatt_X1 = flat(conv_s2(emb_X1))\n",
    "print(flatt_X1.numpy().shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 10)\n",
      "(1, 1)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "dense10 = Dense(10, activation='relu')\n",
    "dense1  = Dense(1, activation='sigmoid')\n",
    "\n",
    "dense10_X1 = dense10(flatt_X1)\n",
    "dense1_X1 =dense1(dense10_X1)\n",
    "\n",
    "print(dense10_X1.numpy().shape)\n",
    "print(dense1_X1.numpy().shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_12 (Embedding)     (None, 512, 10)           210       \n",
      "_________________________________________________________________\n",
      "conv1d_24 (Conv1D)           (None, 256, 32)           1312      \n",
      "_________________________________________________________________\n",
      "layer_normalization_14 (Laye (None, 256, 32)           64        \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 256, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_25 (Conv1D)           (None, 128, 64)           8256      \n",
      "_________________________________________________________________\n",
      "layer_normalization_15 (Laye (None, 128, 64)           128       \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 128, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_26 (Conv1D)           (None, 64, 64)            16448     \n",
      "_________________________________________________________________\n",
      "layer_normalization_16 (Laye (None, 64, 64)            128       \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 4097      \n",
      "=================================================================\n",
      "Total params: 30,643\n",
      "Trainable params: 30,643\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(VOCABELARY_SIZE, EMBEDDING_SIZE, input_shape=(MAX_SEQ_LEN,)))\n",
    "model.add(Conv1D(32,4,padding='same',strides=2, activation='relu'))\n",
    "model.add(LayerNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv1D(64,4,padding='same',strides=2, activation='relu'))\n",
    "model.add(LayerNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv1D(64,4,padding='same',strides=2, activation='relu'))\n",
    "model.add(LayerNormalization())\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='linear'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "\n",
    "def coef_det_k(y_true, y_pred):\n",
    "    \"\"\"Computer coefficient of determination R^2\n",
    "    \"\"\"\n",
    "    SS_res = K.sum(K.square(y_true - y_pred))\n",
    "    SS_tot = K.sum(K.square(y_true - K.mean(y_true)))\n",
    "    return 1 - SS_res / (SS_tot + K.epsilon())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1048 samples, validate on 262 samples\n",
      "Epoch 1/100\n",
      "1048/1048 [==============================] - 1s 1ms/sample - loss: 997.7913 - coef_det_k: -1.8972 - val_loss: 391.9526 - val_coef_det_k: -0.1145\n",
      "Epoch 2/100\n",
      "1048/1048 [==============================] - 0s 137us/sample - loss: 372.5923 - coef_det_k: -0.0650 - val_loss: 395.1577 - val_coef_det_k: -0.1507\n",
      "Epoch 3/100\n",
      "1048/1048 [==============================] - 0s 136us/sample - loss: 364.5835 - coef_det_k: -0.0576 - val_loss: 390.4364 - val_coef_det_k: -0.1148\n",
      "Epoch 4/100\n",
      "1048/1048 [==============================] - 0s 136us/sample - loss: 357.5040 - coef_det_k: -0.0289 - val_loss: 390.8693 - val_coef_det_k: -0.1289\n",
      "Epoch 5/100\n",
      "1048/1048 [==============================] - 0s 134us/sample - loss: 358.2779 - coef_det_k: -0.0302 - val_loss: 394.4902 - val_coef_det_k: -0.1159\n",
      "Epoch 6/100\n",
      "1048/1048 [==============================] - 0s 134us/sample - loss: 364.5722 - coef_det_k: -0.0568 - val_loss: 387.4503 - val_coef_det_k: -0.1067\n",
      "Epoch 7/100\n",
      "1048/1048 [==============================] - 0s 136us/sample - loss: 357.6709 - coef_det_k: -0.0320 - val_loss: 412.5942 - val_coef_det_k: -0.1463\n",
      "Epoch 8/100\n",
      "1048/1048 [==============================] - 0s 135us/sample - loss: 347.1336 - coef_det_k: 3.4575e-04 - val_loss: 374.6189 - val_coef_det_k: -0.0690\n",
      "Epoch 9/100\n",
      "1048/1048 [==============================] - 0s 135us/sample - loss: 352.8127 - coef_det_k: -0.0352 - val_loss: 379.5988 - val_coef_det_k: -0.0523\n",
      "Epoch 10/100\n",
      "1048/1048 [==============================] - 0s 135us/sample - loss: 325.9362 - coef_det_k: 0.0564 - val_loss: 341.3653 - val_coef_det_k: 0.0371\n",
      "Epoch 11/100\n",
      "1048/1048 [==============================] - 0s 134us/sample - loss: 319.7975 - coef_det_k: 0.0098 - val_loss: 370.3979 - val_coef_det_k: -0.0030\n",
      "Epoch 12/100\n",
      "1048/1048 [==============================] - 0s 136us/sample - loss: 293.7517 - coef_det_k: 0.1236 - val_loss: 352.5494 - val_coef_det_k: 0.0466\n",
      "Epoch 13/100\n",
      "1048/1048 [==============================] - 0s 135us/sample - loss: 267.9362 - coef_det_k: 0.2097 - val_loss: 305.4903 - val_coef_det_k: 0.1486\n",
      "Epoch 14/100\n",
      "1048/1048 [==============================] - 0s 136us/sample - loss: 237.6593 - coef_det_k: 0.3047 - val_loss: 315.2773 - val_coef_det_k: 0.1334\n",
      "Epoch 15/100\n",
      "1048/1048 [==============================] - 0s 135us/sample - loss: 222.9016 - coef_det_k: 0.3432 - val_loss: 329.4885 - val_coef_det_k: 0.0924\n",
      "Epoch 16/100\n",
      "1048/1048 [==============================] - 0s 135us/sample - loss: 211.1233 - coef_det_k: 0.3749 - val_loss: 288.9722 - val_coef_det_k: 0.1625\n",
      "Epoch 17/100\n",
      "1048/1048 [==============================] - 0s 134us/sample - loss: 197.4777 - coef_det_k: 0.3931 - val_loss: 284.3856 - val_coef_det_k: 0.1719\n",
      "Epoch 18/100\n",
      "1048/1048 [==============================] - 0s 135us/sample - loss: 195.7979 - coef_det_k: 0.4001 - val_loss: 296.3002 - val_coef_det_k: 0.1195\n",
      "Epoch 19/100\n",
      "1048/1048 [==============================] - 0s 134us/sample - loss: 156.5793 - coef_det_k: 0.5326 - val_loss: 295.1216 - val_coef_det_k: 0.1401\n",
      "Epoch 20/100\n",
      "1048/1048 [==============================] - 0s 134us/sample - loss: 145.8024 - coef_det_k: 0.5574 - val_loss: 287.5630 - val_coef_det_k: 0.1500\n",
      "Epoch 21/100\n",
      "1048/1048 [==============================] - 0s 135us/sample - loss: 128.9556 - coef_det_k: 0.5881 - val_loss: 308.2726 - val_coef_det_k: 0.1031\n",
      "Epoch 22/100\n",
      "1048/1048 [==============================] - 0s 134us/sample - loss: 113.3061 - coef_det_k: 0.6556 - val_loss: 295.0568 - val_coef_det_k: 0.1072\n",
      "Epoch 23/100\n",
      "1048/1048 [==============================] - 0s 135us/sample - loss: 104.3480 - coef_det_k: 0.6817 - val_loss: 288.4177 - val_coef_det_k: 0.1316\n",
      "Epoch 24/100\n",
      "1048/1048 [==============================] - 0s 134us/sample - loss: 87.5052 - coef_det_k: 0.7232 - val_loss: 308.0929 - val_coef_det_k: 0.1017\n",
      "Epoch 25/100\n",
      "1048/1048 [==============================] - 0s 135us/sample - loss: 80.7150 - coef_det_k: 0.7561 - val_loss: 296.0096 - val_coef_det_k: 0.1076\n",
      "Epoch 26/100\n",
      "1048/1048 [==============================] - 0s 135us/sample - loss: 73.2308 - coef_det_k: 0.7817 - val_loss: 294.8607 - val_coef_det_k: 0.1192\n",
      "Epoch 27/100\n",
      "1048/1048 [==============================] - 0s 135us/sample - loss: 66.8791 - coef_det_k: 0.8039 - val_loss: 298.0054 - val_coef_det_k: 0.1082\n",
      "Epoch 28/100\n",
      "1048/1048 [==============================] - 0s 134us/sample - loss: 60.2331 - coef_det_k: 0.8214 - val_loss: 295.0510 - val_coef_det_k: 0.1157\n",
      "Epoch 29/100\n",
      "1048/1048 [==============================] - 0s 135us/sample - loss: 55.6297 - coef_det_k: 0.8280 - val_loss: 305.0105 - val_coef_det_k: 0.0877\n",
      "Epoch 30/100\n",
      "1048/1048 [==============================] - 0s 135us/sample - loss: 52.3281 - coef_det_k: 0.8364 - val_loss: 304.8323 - val_coef_det_k: 0.0798\n",
      "Epoch 31/100\n",
      "1048/1048 [==============================] - 0s 134us/sample - loss: 46.4369 - coef_det_k: 0.8601 - val_loss: 316.7119 - val_coef_det_k: 0.0441\n",
      "Epoch 32/100\n",
      "1048/1048 [==============================] - 0s 135us/sample - loss: 46.6217 - coef_det_k: 0.8615 - val_loss: 313.2468 - val_coef_det_k: 0.0623\n",
      "Epoch 33/100\n",
      "1048/1048 [==============================] - 0s 134us/sample - loss: 42.8984 - coef_det_k: 0.8638 - val_loss: 316.9425 - val_coef_det_k: 0.0712\n",
      "Epoch 34/100\n",
      "1048/1048 [==============================] - 0s 134us/sample - loss: 42.2968 - coef_det_k: 0.8674 - val_loss: 326.3842 - val_coef_det_k: 0.0376\n",
      "Epoch 35/100\n",
      "1048/1048 [==============================] - 0s 136us/sample - loss: 40.9445 - coef_det_k: 0.8793 - val_loss: 312.3250 - val_coef_det_k: 0.0556\n",
      "Epoch 36/100\n",
      "1048/1048 [==============================] - 0s 135us/sample - loss: 40.3078 - coef_det_k: 0.8757 - val_loss: 313.2673 - val_coef_det_k: 0.0666\n",
      "Epoch 37/100\n",
      "1048/1048 [==============================] - 0s 135us/sample - loss: 39.1316 - coef_det_k: 0.8737 - val_loss: 341.4531 - val_coef_det_k: 0.0026\n",
      "Epoch 38/100\n",
      "1048/1048 [==============================] - 0s 134us/sample - loss: 41.7549 - coef_det_k: 0.8726 - val_loss: 320.3654 - val_coef_det_k: 0.0580\n",
      "Epoch 39/100\n",
      "1048/1048 [==============================] - 0s 134us/sample - loss: 35.0671 - coef_det_k: 0.8948 - val_loss: 317.8105 - val_coef_det_k: 0.0673\n",
      "Epoch 40/100\n",
      "1048/1048 [==============================] - 0s 135us/sample - loss: 32.1850 - coef_det_k: 0.9016 - val_loss: 314.6475 - val_coef_det_k: 0.0735\n",
      "Epoch 41/100\n",
      "1048/1048 [==============================] - 0s 136us/sample - loss: 30.5484 - coef_det_k: 0.9052 - val_loss: 305.5751 - val_coef_det_k: 0.0891\n",
      "Epoch 42/100\n",
      "1048/1048 [==============================] - 0s 135us/sample - loss: 31.1746 - coef_det_k: 0.9030 - val_loss: 306.5361 - val_coef_det_k: 0.0814\n",
      "Epoch 43/100\n",
      "1048/1048 [==============================] - 0s 137us/sample - loss: 34.4470 - coef_det_k: 0.8970 - val_loss: 319.1589 - val_coef_det_k: 0.0578\n",
      "Epoch 44/100\n",
      "1048/1048 [==============================] - 0s 134us/sample - loss: 30.8764 - coef_det_k: 0.9020 - val_loss: 305.7777 - val_coef_det_k: 0.0767\n",
      "Epoch 45/100\n",
      "1048/1048 [==============================] - 0s 135us/sample - loss: 29.9308 - coef_det_k: 0.9090 - val_loss: 306.5734 - val_coef_det_k: 0.0868\n",
      "Epoch 46/100\n",
      "1048/1048 [==============================] - 0s 135us/sample - loss: 31.4082 - coef_det_k: 0.9066 - val_loss: 305.1596 - val_coef_det_k: 0.0867\n",
      "Epoch 47/100\n",
      "1048/1048 [==============================] - 0s 134us/sample - loss: 28.6988 - coef_det_k: 0.9124 - val_loss: 313.9515 - val_coef_det_k: 0.0685\n",
      "Epoch 48/100\n",
      "1048/1048 [==============================] - 0s 134us/sample - loss: 29.7803 - coef_det_k: 0.9112 - val_loss: 306.3752 - val_coef_det_k: 0.0804\n",
      "Epoch 49/100\n",
      "1048/1048 [==============================] - 0s 134us/sample - loss: 32.5019 - coef_det_k: 0.8994 - val_loss: 308.8831 - val_coef_det_k: 0.0774\n",
      "Epoch 50/100\n",
      "1048/1048 [==============================] - 0s 135us/sample - loss: 30.3852 - coef_det_k: 0.9075 - val_loss: 315.2544 - val_coef_det_k: 0.0403\n",
      "Epoch 51/100\n",
      "1048/1048 [==============================] - 0s 135us/sample - loss: 28.4846 - coef_det_k: 0.9127 - val_loss: 310.1400 - val_coef_det_k: 0.0776\n",
      "Epoch 52/100\n",
      "1048/1048 [==============================] - 0s 134us/sample - loss: 27.2757 - coef_det_k: 0.9158 - val_loss: 319.0388 - val_coef_det_k: 0.0640\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/100\n",
      "1048/1048 [==============================] - 0s 135us/sample - loss: 28.9232 - coef_det_k: 0.9101 - val_loss: 303.9345 - val_coef_det_k: 0.0818\n",
      "Epoch 54/100\n",
      "1048/1048 [==============================] - 0s 135us/sample - loss: 28.4336 - coef_det_k: 0.9113 - val_loss: 304.7028 - val_coef_det_k: 0.0800\n",
      "Epoch 55/100\n",
      "1048/1048 [==============================] - 0s 136us/sample - loss: 26.1304 - coef_det_k: 0.9190 - val_loss: 306.6574 - val_coef_det_k: 0.0726\n",
      "Epoch 56/100\n",
      "1048/1048 [==============================] - 0s 135us/sample - loss: 28.5644 - coef_det_k: 0.9118 - val_loss: 307.7262 - val_coef_det_k: 0.0780\n",
      "Epoch 57/100\n",
      "1048/1048 [==============================] - 0s 134us/sample - loss: 29.0173 - coef_det_k: 0.9104 - val_loss: 308.5167 - val_coef_det_k: 0.0817\n",
      "Epoch 58/100\n",
      "1048/1048 [==============================] - 0s 136us/sample - loss: 25.5682 - coef_det_k: 0.9224 - val_loss: 303.9108 - val_coef_det_k: 0.0974\n",
      "Epoch 59/100\n",
      "1048/1048 [==============================] - 0s 135us/sample - loss: 26.1512 - coef_det_k: 0.9213 - val_loss: 306.7209 - val_coef_det_k: 0.0825\n",
      "Epoch 60/100\n",
      "1048/1048 [==============================] - 0s 135us/sample - loss: 26.1706 - coef_det_k: 0.9191 - val_loss: 317.3990 - val_coef_det_k: 0.0640\n",
      "Epoch 61/100\n",
      "1048/1048 [==============================] - 0s 135us/sample - loss: 26.1824 - coef_det_k: 0.9182 - val_loss: 313.3186 - val_coef_det_k: 0.0662\n",
      "Epoch 62/100\n",
      "1048/1048 [==============================] - 0s 134us/sample - loss: 25.1812 - coef_det_k: 0.9249 - val_loss: 311.0309 - val_coef_det_k: 0.0617\n",
      "Epoch 63/100\n",
      "1048/1048 [==============================] - 0s 135us/sample - loss: 25.0041 - coef_det_k: 0.9219 - val_loss: 315.5683 - val_coef_det_k: 0.0754\n",
      "Epoch 64/100\n",
      "1048/1048 [==============================] - 0s 135us/sample - loss: 24.3138 - coef_det_k: 0.9269 - val_loss: 311.8746 - val_coef_det_k: 0.0791\n",
      "Epoch 65/100\n",
      "1048/1048 [==============================] - 0s 135us/sample - loss: 24.2425 - coef_det_k: 0.9245 - val_loss: 312.3359 - val_coef_det_k: 0.0888\n",
      "Epoch 66/100\n",
      "1048/1048 [==============================] - 0s 135us/sample - loss: 25.7783 - coef_det_k: 0.9190 - val_loss: 313.6236 - val_coef_det_k: 0.0834\n",
      "Epoch 67/100\n",
      "1048/1048 [==============================] - 0s 134us/sample - loss: 25.3698 - coef_det_k: 0.9230 - val_loss: 309.2401 - val_coef_det_k: 0.0818\n",
      "Epoch 68/100\n",
      "1048/1048 [==============================] - 0s 135us/sample - loss: 26.2502 - coef_det_k: 0.9176 - val_loss: 308.6272 - val_coef_det_k: 0.0781\n",
      "Epoch 69/100\n",
      "1048/1048 [==============================] - 0s 135us/sample - loss: 21.8278 - coef_det_k: 0.9334 - val_loss: 313.1471 - val_coef_det_k: 0.0763\n",
      "Epoch 70/100\n",
      "1048/1048 [==============================] - 0s 134us/sample - loss: 23.5986 - coef_det_k: 0.9294 - val_loss: 312.5401 - val_coef_det_k: 0.0692\n",
      "Epoch 71/100\n",
      "1048/1048 [==============================] - 0s 134us/sample - loss: 23.1993 - coef_det_k: 0.9287 - val_loss: 313.3334 - val_coef_det_k: 0.0691\n",
      "Epoch 72/100\n",
      "1048/1048 [==============================] - 0s 136us/sample - loss: 22.4832 - coef_det_k: 0.9315 - val_loss: 309.2097 - val_coef_det_k: 0.0704\n",
      "Epoch 73/100\n",
      "1048/1048 [==============================] - 0s 135us/sample - loss: 24.0234 - coef_det_k: 0.9258 - val_loss: 314.8713 - val_coef_det_k: 0.0389\n",
      "Epoch 74/100\n",
      "1048/1048 [==============================] - 0s 135us/sample - loss: 22.7472 - coef_det_k: 0.9307 - val_loss: 307.2232 - val_coef_det_k: 0.0730\n",
      "Epoch 75/100\n",
      "1048/1048 [==============================] - 0s 135us/sample - loss: 22.1758 - coef_det_k: 0.9337 - val_loss: 310.0378 - val_coef_det_k: 0.0812\n",
      "Epoch 76/100\n",
      "1048/1048 [==============================] - 0s 134us/sample - loss: 23.0190 - coef_det_k: 0.9306 - val_loss: 305.5842 - val_coef_det_k: 0.0857\n",
      "Epoch 77/100\n",
      "1048/1048 [==============================] - 0s 135us/sample - loss: 22.9521 - coef_det_k: 0.9289 - val_loss: 306.4400 - val_coef_det_k: 0.0831\n",
      "Epoch 78/100\n",
      "1048/1048 [==============================] - 0s 135us/sample - loss: 20.7964 - coef_det_k: 0.9366 - val_loss: 303.9788 - val_coef_det_k: 0.0946\n",
      "Epoch 79/100\n",
      "1048/1048 [==============================] - 0s 134us/sample - loss: 21.2260 - coef_det_k: 0.9334 - val_loss: 309.6051 - val_coef_det_k: 0.0819\n",
      "Epoch 80/100\n",
      "1048/1048 [==============================] - 0s 135us/sample - loss: 23.7186 - coef_det_k: 0.9271 - val_loss: 307.7449 - val_coef_det_k: 0.0895\n",
      "Epoch 81/100\n",
      "1048/1048 [==============================] - 0s 135us/sample - loss: 22.1921 - coef_det_k: 0.9313 - val_loss: 310.8352 - val_coef_det_k: 0.0832\n",
      "Epoch 82/100\n",
      "1048/1048 [==============================] - 0s 135us/sample - loss: 21.8105 - coef_det_k: 0.9339 - val_loss: 310.4989 - val_coef_det_k: 0.0795\n",
      "Epoch 83/100\n",
      "1048/1048 [==============================] - 0s 135us/sample - loss: 22.8171 - coef_det_k: 0.9308 - val_loss: 309.4935 - val_coef_det_k: 0.0904\n",
      "Epoch 84/100\n",
      "1048/1048 [==============================] - 0s 135us/sample - loss: 21.0842 - coef_det_k: 0.9349 - val_loss: 304.1413 - val_coef_det_k: 0.0910\n",
      "Epoch 85/100\n",
      "1048/1048 [==============================] - 0s 135us/sample - loss: 20.4793 - coef_det_k: 0.9376 - val_loss: 304.7965 - val_coef_det_k: 0.0900\n",
      "Epoch 86/100\n",
      "1048/1048 [==============================] - 0s 135us/sample - loss: 22.2051 - coef_det_k: 0.9317 - val_loss: 305.5278 - val_coef_det_k: 0.0764\n",
      "Epoch 87/100\n",
      "1048/1048 [==============================] - 0s 134us/sample - loss: 21.0818 - coef_det_k: 0.9345 - val_loss: 317.9554 - val_coef_det_k: 0.0633\n",
      "Epoch 88/100\n",
      "1048/1048 [==============================] - 0s 135us/sample - loss: 21.1470 - coef_det_k: 0.9335 - val_loss: 327.0326 - val_coef_det_k: 0.0442\n",
      "Epoch 89/100\n",
      "1048/1048 [==============================] - 0s 134us/sample - loss: 21.1688 - coef_det_k: 0.9351 - val_loss: 311.6145 - val_coef_det_k: 0.0768\n",
      "Epoch 90/100\n",
      "1048/1048 [==============================] - 0s 134us/sample - loss: 19.7317 - coef_det_k: 0.9369 - val_loss: 311.9142 - val_coef_det_k: 0.0704\n",
      "Epoch 91/100\n",
      "1048/1048 [==============================] - 0s 134us/sample - loss: 19.3651 - coef_det_k: 0.9407 - val_loss: 311.4348 - val_coef_det_k: 0.0754\n",
      "Epoch 92/100\n",
      "1048/1048 [==============================] - 0s 135us/sample - loss: 20.5551 - coef_det_k: 0.9371 - val_loss: 307.4922 - val_coef_det_k: 0.0709\n",
      "Epoch 93/100\n",
      "1048/1048 [==============================] - 0s 136us/sample - loss: 22.1614 - coef_det_k: 0.9298 - val_loss: 302.4040 - val_coef_det_k: 0.0995\n",
      "Epoch 94/100\n",
      "1048/1048 [==============================] - 0s 135us/sample - loss: 19.1261 - coef_det_k: 0.9424 - val_loss: 304.3225 - val_coef_det_k: 0.0953\n",
      "Epoch 95/100\n",
      "1048/1048 [==============================] - 0s 135us/sample - loss: 22.1546 - coef_det_k: 0.9335 - val_loss: 309.1142 - val_coef_det_k: 0.0895\n",
      "Epoch 96/100\n",
      "1048/1048 [==============================] - 0s 135us/sample - loss: 19.6151 - coef_det_k: 0.9393 - val_loss: 305.5561 - val_coef_det_k: 0.0919\n",
      "Epoch 97/100\n",
      "1048/1048 [==============================] - 0s 134us/sample - loss: 20.0362 - coef_det_k: 0.9395 - val_loss: 315.2532 - val_coef_det_k: 0.0645\n",
      "Epoch 98/100\n",
      "1048/1048 [==============================] - 0s 136us/sample - loss: 21.4080 - coef_det_k: 0.9325 - val_loss: 310.5421 - val_coef_det_k: 0.0538\n",
      "Epoch 99/100\n",
      "1048/1048 [==============================] - 0s 135us/sample - loss: 22.0886 - coef_det_k: 0.9329 - val_loss: 303.2353 - val_coef_det_k: 0.1047\n",
      "Epoch 100/100\n",
      "1048/1048 [==============================] - 0s 135us/sample - loss: 20.2495 - coef_det_k: 0.9371 - val_loss: 306.6076 - val_coef_det_k: 0.0921\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "\n",
    "opt = Adam(learning_rate = 0.001)\n",
    "loss = MeanSquaredError()\n",
    "model.compile(optimizer=opt, loss=loss, metrics=[coef_det_k])\n",
    "history = model.fit(X_train, y_train, epochs= 100, batch_size = 32, validation_data=(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['loss', 'coef_det_k', 'val_loss', 'val_coef_det_k'])\n"
     ]
    }
   ],
   "source": [
    "# Plot history\n",
    "\n",
    "print(history.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "ax1 = plt.subplot(1,2,1)\n",
    "ax2 = plt.subplot(1,2,2)\n",
    "\n",
    "ax1.plot(history.history['loss'])\n",
    "ax1.plot(history.history['val_loss'])\n",
    "ax1.legend(['Training loss', 'Validation loss'])\n",
    "\n",
    "ax2.plot(history.history['coef_det_k'])\n",
    "ax2.plot(history.history['val_coef_det_k'])\n",
    "ax2.legend(['Training R^2', 'Validation R^2'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make your own model\n",
    "Testing your own model architectures is probably the best way to learn some intuitoin of how different parameters affect your model performance. It is important to note that you should not evaluate your model performance based on the training loss or training R^2 but rather the Validation scores. Try to make your own model that gets as high Validation scores as posible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model = Sequential()\n",
    "\n",
    "my_model.add(Embedding(,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
